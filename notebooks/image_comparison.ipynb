{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-fidelity\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d10d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch_fidelity import calculate_metrics\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "\n",
    "def reservoir_sample_dir(root_dir, sample_size, exts=(\".png\", \".jpg\", \".jpeg\")):\n",
    "    reservoir = []\n",
    "    for i, entry in enumerate(os.scandir(root_dir)):\n",
    "        if not entry.is_file():\n",
    "            continue\n",
    "        name = entry.name.lower()\n",
    "        if not name.endswith(exts):\n",
    "            continue\n",
    "        path = entry.path\n",
    "        if len(reservoir) < sample_size:\n",
    "            reservoir.append(path)\n",
    "        else:\n",
    "            j = random.randint(0, i)\n",
    "            if j < sample_size:\n",
    "                reservoir[j] = path\n",
    "    return reservoir\n",
    "\n",
    "class PathListDataset(Dataset):\n",
    "    def __init__(self, paths, size=(299, 299)):\n",
    "        self.paths = paths\n",
    "        self.size  = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        img = img.resize(self.size, resample=Image.BILINEAR)\n",
    "        arr = np.array(img, dtype=np.uint8)    # H×W×C\n",
    "        t   = torch.from_numpy(arr)            # ByteTensor H×W×C\n",
    "        return t.permute(2, 0, 1)              # ByteTensor C×H×W\n",
    "\n",
    "\n",
    "real_dir = \"/content/drive/MyDrive/images2/images2\"\n",
    "gen_dir  = \"/content/drive/MyDrive/generated_images\"\n",
    "\n",
    "sample_n     = 50000\n",
    "real_sample  = reservoir_sample_dir(real_dir, sample_n)\n",
    "gen_sample   = reservoir_sample_dir(gen_dir,  sample_n)\n",
    "\n",
    "print(f\"Sampled {len(real_sample)} real paths, {len(gen_sample)} gen paths.\")\n",
    "\n",
    "ds_real_samp = PathListDataset(real_sample)\n",
    "ds_gen_samp  = PathListDataset(gen_sample)\n",
    "\n",
    "metrics = calculate_metrics(\n",
    "    input1=ds_real_samp,\n",
    "    input2=ds_gen_samp,\n",
    "    cuda=True,\n",
    "    isc=False,\n",
    "    fid=True,\n",
    "    kid=True,\n",
    "    num_workers=8,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "print(\"Frechet Inception Distance:\", metrics[\"frechet_inception_distance\"])\n",
    "print(\n",
    "    \"Kernel Inception Distance:\",\n",
    "    f\"{metrics['kernel_inception_distance_mean']:.6f} ± \"\n",
    "    f\"{metrics['kernel_inception_distance_std']:.6f}\"\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def get_clip_embeddings(paths, batch_size=32):\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(paths), batch_size), desc=\"CLIP batches\"):\n",
    "        batch = [preprocess(Image.open(p).convert(\"RGB\")) for p in paths[i : i + batch_size]]\n",
    "        bt    = torch.stack(batch).to(device, non_blocking=True)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            e = model.encode_image(bt)\n",
    "            e = e / e.norm(dim=-1, keepdim=True)\n",
    "        embs.append(e.cpu())\n",
    "    return torch.cat(embs, dim=0)\n",
    "\n",
    "def median_heuristic_sigma(x, y):\n",
    "    with torch.no_grad():\n",
    "        z      = torch.cat([x, y], dim=0)\n",
    "        sample = z[torch.randperm(len(z))][: min(len(z), 1000)]\n",
    "        d2     = torch.cdist(sample, sample, p=2).reshape(-1)\n",
    "        return torch.median(d2).item()\n",
    "\n",
    "def compute_mmd(x, y, sigma=None):\n",
    "    m, n = x.size(0), y.size(0)\n",
    "    if sigma is None:\n",
    "        sigma = median_heuristic_sigma(x, y)\n",
    "    xx = torch.cdist(x, x, p=2).pow(2)\n",
    "    yy = torch.cdist(y, y, p=2).pow(2)\n",
    "    xy = torch.cdist(x, y, p=2).pow(2)\n",
    "    Kxx = torch.exp(-xx / (2 * sigma ** 2))\n",
    "    Kyy = torch.exp(-yy / (2 * sigma ** 2))\n",
    "    Kxy = torch.exp(-xy / (2 * sigma ** 2))\n",
    "    term_x  = (Kxx.sum() - Kxx.trace()) / (m * (m - 1))\n",
    "    term_y  = (Kyy.sum() - Kyy.trace()) / (n * (n - 1))\n",
    "    term_xy = 2 * Kxy.sum() / (m * n)\n",
    "    return term_x + term_y - term_xy\n",
    "\n",
    "print(\"\\nComputing CLIP embeddings for real images...\")\n",
    "real_emb = get_clip_embeddings(real_sample)\n",
    "\n",
    "print(\"\\nComputing CLIP embeddings for generated images...\")\n",
    "gen_emb = get_clip_embeddings(gen_sample)\n",
    "\n",
    "sigma = median_heuristic_sigma(real_emb, gen_emb)\n",
    "print(f\"Using sigma = {sigma:.4f}\")\n",
    "\n",
    "cmmd2 = compute_mmd(real_emb, gen_emb, sigma=sigma)\n",
    "print(f\"CMMD² (CLIP-based MMD): {cmmd2:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
