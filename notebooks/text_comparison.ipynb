{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert-score\n",
    "!pip install moverscore\n",
    "!pip install pycocoevalcap\n",
    "!pip install pyemd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e43a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "bert_f1_list = []\n",
    "ref_texts = []\n",
    "cand_texts = []\n",
    "\n",
    "total_samples = 0\n",
    "sum_bert_f1 = 0.0\n",
    "\n",
    "chunksize = 1000  # Adjust chunksize based on available memory\n",
    "csv_file = '/content/drive/MyDrive/updated_annotations.csv' # your CSV file which contains original and generated captions\n",
    "count = 0\n",
    "\n",
    "for chunk in pd.read_csv(csv_file, chunksize=chunksize):\n",
    "    # Extract lists of texts for batch processing\n",
    "    chunk_refs = chunk['title_multi_objects'].tolist()\n",
    "    chunk_cands = chunk['generated_captions'].tolist()\n",
    "\n",
    "    # Save texts for later DataFrame construction\n",
    "    ref_texts.extend(chunk_refs)\n",
    "    cand_texts.extend(chunk_cands)\n",
    "\n",
    "    # Calculate BERTScore for the entire chunk using CUDA if available\n",
    "    _, _, bert_f1_chunk = bert_score(\n",
    "        chunk_cands,\n",
    "        chunk_refs,\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "        device=device,\n",
    "        batch_size=128  # Increase batch_size if memory permits\n",
    "    )\n",
    "\n",
    "    # Convert tensor to list and update cumulative values\n",
    "    bert_f1_chunk = bert_f1_chunk.tolist()\n",
    "    bert_f1_list.extend(bert_f1_chunk)\n",
    "    sum_bert_f1 += sum(bert_f1_chunk)\n",
    "    total_samples += len(bert_f1_chunk)\n",
    "\n",
    "    # Clean up memory for the processed chunk\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Processed chunk:\", count)\n",
    "    count += 1\n",
    "\n",
    "# Create a DataFrame from the accumulated lists\n",
    "result_df = pd.DataFrame({\n",
    "    'title_multi_objects': ref_texts,\n",
    "    'generated_captions': cand_texts,\n",
    "    'BERT-F1': bert_f1_list\n",
    "})\n",
    "\n",
    "print(result_df[['title_multi_objects', 'generated_captions', 'BERT-F1']].head())\n",
    "\n",
    "# Calculate average BERT-F1 value\n",
    "avg_bert_f1 = sum_bert_f1 / total_samples\n",
    "print(\"Dataset Average Values:\")\n",
    "print(f\"  BERT-F1 : {avg_bert_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fa438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "ref_texts = []\n",
    "cand_texts = []\n",
    "chunksize = 1000\n",
    "csv_file = '/content/drive/MyDrive/updated_annotations.csv' # your CSV file which contains original and generated captions\n",
    "count = 0\n",
    "\n",
    "for chunk in pd.read_csv(csv_file, chunksize=chunksize):\n",
    "    ref_texts.extend(chunk['title_multi_objects'].tolist())\n",
    "    cand_texts.extend(chunk['generated_captions'].tolist())\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "    print(\"Processed chunk:\", count)\n",
    "    count += 1\n",
    "\n",
    "# filter out empty or NaN values\n",
    "filtered_refs = []\n",
    "filtered_cands = []\n",
    "for ref, cand in zip(ref_texts, cand_texts):\n",
    "    if pd.notna(ref) and pd.notna(cand) and str(ref).strip() and str(cand).strip():\n",
    "        filtered_refs.append(str(ref).strip())\n",
    "        filtered_cands.append(str(cand).strip())\n",
    "\n",
    "print(\"Number of valid pairs:\", len(filtered_refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a5f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor.tokenizer.model_max_length = 128\n",
    "\n",
    "new_max_length = 128\n",
    "old_max_length = clip_model.config.text_config.max_position_embeddings  \n",
    "hidden_size = clip_model.text_model.embeddings.position_embedding.weight.shape[1]\n",
    "old_pos_embed = clip_model.text_model.embeddings.position_embedding.weight.data\n",
    "if new_max_length > old_max_length:\n",
    "    print(f\"Extending positional embeddings from {old_max_length} to {new_max_length}\")\n",
    "    new_pos_embed = torch.zeros(new_max_length, hidden_size, device=old_pos_embed.device)\n",
    "    new_pos_embed[:old_max_length, :] = old_pos_embed\n",
    "    new_pos_embed[old_max_length:, :] = old_pos_embed[-1, :].unsqueeze(0).expand(new_max_length - old_max_length, hidden_size)\n",
    "    new_embedding = torch.nn.Embedding(new_max_length, hidden_size).to(device)\n",
    "    new_embedding.weight.data = new_pos_embed\n",
    "    clip_model.text_model.embeddings.position_embedding = new_embedding\n",
    "    clip_model.config.text_config.max_position_embeddings = new_max_length\n",
    "\n",
    "batch_size = 32\n",
    "clip_scores = []\n",
    "num_batches = (len(filtered_refs) + batch_size - 1) // batch_size\n",
    "\n",
    "for i in range(0, len(filtered_refs), batch_size):\n",
    "    batch_refs = filtered_refs[i:i+batch_size]\n",
    "    batch_cands = filtered_cands[i:i+batch_size]\n",
    "    texts = []\n",
    "    for ref, cand in zip(batch_refs, batch_cands):\n",
    "        texts.append(ref)\n",
    "        texts.append(cand)\n",
    "    inputs = clip_processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    position_ids = torch.arange(input_ids.shape[1], dtype=torch.long, device=device).unsqueeze(0).expand_as(input_ids)\n",
    "    inputs[\"position_ids\"] = position_ids\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(**inputs)\n",
    "\n",
    "    for j in range(0, text_features.shape[0], 2):\n",
    "        score = torch.nn.functional.cosine_similarity(text_features[j:j+1], text_features[j+1:j+2]).item()\n",
    "        clip_scores.append(score)\n",
    "    print(f\"Processed batch {i//batch_size + 1}/{num_batches}\")\n",
    "\n",
    "avg_clip_score = sum(clip_scores) / len(clip_scores)\n",
    "print(\"Average CLIPScore: {:.4f}\".format(avg_clip_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
